{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\"\"\"GCN using DGL nn package\n",
    "\n",
    "References:\n",
    "- Semi-Supervised Classification with Graph Convolutional Networks\n",
    "- Paper: https://arxiv.org/abs/1609.02907\n",
    "- Code: https://github.com/tkipf/gcn\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 g,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 n_classes,\n",
    "                 n_layers,\n",
    "                 activation,\n",
    "                 dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.g = g\n",
    "        self.layers = nn.ModuleList()\n",
    "        # input layer\n",
    "        self.layers.append(GraphConv(in_feats, n_hidden, activation=activation))\n",
    "        # hidden layers\n",
    "        for i in range(n_layers - 1):\n",
    "            self.layers.append(GraphConv(n_hidden, n_hidden, activation=activation))\n",
    "        # output layer\n",
    "        self.layers.append(GraphConv(n_hidden, n_classes))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, features):\n",
    "        h = features\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i != 0:\n",
    "                h = self.dropout(h)\n",
    "            h = layer(self.g, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "from dgl.data import load_data\n",
    "import networkx as nx\n",
    "import os\n",
    "import sys\n",
    "thismodule = sys.modules[__name__]\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "class Args(object):\n",
    "    pass\n",
    "\n",
    "\n",
    "class InnerProductDecoder(torch.nn.Module):\n",
    "    def __init__(self, activation=torch.sigmoid, dropout=0.1):\n",
    "        super(InnerProductDecoder, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = F.dropout(z, self.dropout)\n",
    "        adj = self.activation(torch.mm(z, z.t()))\n",
    "        return adj\n",
    "\n",
    "# crossentropy = torch.nn.CrossEntropyLoss()\n",
    "def lp_loss(logits, labels, edges):\n",
    "    preds = torch.sum(logits[edges[:, 0]] * logits[edges[:, 1]], dim=1)\n",
    "    return F.binary_cross_entropy_with_logits(preds, labels) + 0.1 * torch.mean(logits.pow(2))\n",
    "\n",
    "\n",
    "def unsup_loss(logits, labels, edges, pos_weight):\n",
    "    preds = torch.sigmoid(torch.sum(logits[edges[:, 0]] * logits[edges[:, 1]], dim=1))\n",
    "#     print(preds.min(), preds.max())\n",
    "    return torch.where(labels, - torch.log(preds), - torch.log(1 - preds + 1e-20)).sum()\n",
    "#     torch.log(preds)\n",
    "\n",
    "def accuracy(logits, labels):\n",
    "    _, indices = torch.max(logits, dim=1)\n",
    "    correct = torch.sum(indices == labels.type(torch.LongTensor))\n",
    "    return correct.item() * 1.0 / len(labels)\n",
    "    \n",
    "class DataWrapper(object):\n",
    "    def __init__(self, path, checkerPath=None):\n",
    "        data = pd.read_csv(path, sep=' ', header=None)\n",
    "        self.mapping = set(data[0]) | set(data[1])\n",
    "        self.mapping = dict(zip(self.mapping, range(len(self.mapping))))\n",
    "        if checkerPath is not None:\n",
    "            checker = pd.read_csv(checkerPath, header=None)[0].values\n",
    "            self.labels = ~(data[0].astype(str) + ' ' + data[1].astype(str)).isin(checker)\n",
    "        else:\n",
    "            self.labels = data[2] > 0.5\n",
    "        data[0] = data[0].map(self.mapping)\n",
    "        data[1] = data[1].map(self.mapping)\n",
    "        self.graph = nx.from_pandas_edgelist(data, create_using=nx.Graph, source=0, target=1)\n",
    "        self.features = np.zeros(len(self.graph))\n",
    "        self.edgelist = data[[0, 1]]\n",
    "        \n",
    "def accuracy_lp(logits, labels):\n",
    "    return ((torch.sigmoid(logits) > 0.5).type(torch.FloatTensor) == labels).type(torch.FloatTensor).mean().item()\n",
    "\n",
    "\n",
    "def evaluate(model, features, labels, edges, mask, task):\n",
    "    model.eval()\n",
    "    if task == 'nc':\n",
    "        with torch.no_grad():\n",
    "            logits = model(features)\n",
    "            logits = logits[mask]\n",
    "            labels = labels[mask]\n",
    "            return accuracy(logits, labels)\n",
    "    elif task == 'lp':\n",
    "        with torch.no_grad():\n",
    "            logits = model(features)\n",
    "            preds = torch.sum(logits[edges[mask, 0]] * logits[edges[mask, 1]], dim=1)\n",
    "            return accuracy_lp(preds, labels[mask])\n",
    "\n",
    "def get_data(name):\n",
    "    args = Args()\n",
    "    args.dataset = name\n",
    "    if name in ['cora', 'citeseer', 'pubmed']:\n",
    "        return load_data(args)\n",
    "    return DataWrapper(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcnrgs = {\n",
    "    'n_hidden': 64,\n",
    "    'n_layers': 2,\n",
    "    'dropout': 0.5,\n",
    "    'aggregator_type': 'gcn',\n",
    "    'gpu': 1,\n",
    "    'epochs': 300,\n",
    "    'lr': 1e-2,\n",
    "    'weight_decay': 5e-4,\n",
    "    'self_loop': 'store_true'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, exp_name='gat'):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.exp_name = exp_name + '_es_checkpoint.pt'\n",
    "\n",
    "    def step(self, acc, model):\n",
    "        score = acc\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(model)\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(model)\n",
    "            self.counter = 0\n",
    "        return self.early_stop\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        torch.save(model.state_dict(), self.exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_gcn(g, num_feats, n_classes):\n",
    "    args = Args()\n",
    "    for key, val in gcnrgs.items():\n",
    "        setattr(args, key, val)\n",
    "    model = GCN(g,\n",
    "                num_feats,\n",
    "                args.n_hidden,\n",
    "                n_classes,\n",
    "                args.n_layers,\n",
    "                F.relu,\n",
    "                args.dropout)\n",
    "    return model, args\n",
    "\n",
    "def init_model(g, m, num_feats, num_classes):\n",
    "    return getattr(thismodule, f'init_{m}')(g, num_feats, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lp_dataset(G, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    edgelist = np.array(G.edges)\n",
    "    edgelist = np.concatenate([edgelist, np.ones((edgelist.shape[0], 1))], 1)\n",
    "    negs = negative_sampling(G)\n",
    "    edgelist = np.concatenate([edgelist, negs], 0)\n",
    "    labels = edgelist[:, 2]\n",
    "    edgelist = edgelist[:, :2]\n",
    "    perm = np.random.permutation(edgelist.shape[0])\n",
    "    train_mask = perm[:perm.shape[0] // 3 + 1]\n",
    "    val_mask = perm[perm.shape[0] // 3 + 1:perm.shape[0] // 2 + 1]\n",
    "    test_mask = perm[perm.shape[0] // 2 + 1:]\n",
    "    return edgelist, labels, train_mask, val_mask, test_mask\n",
    "\n",
    "def negative_sampling(G):\n",
    "    adj = nx.to_numpy_array(G)\n",
    "    nn = NearestNeighbors(metric='cosine')\n",
    "    nn.fit(adj)\n",
    "    _, res = nn.kneighbors()\n",
    "    negs = []\n",
    "    for idx, i in enumerate(res):\n",
    "        for j in i:\n",
    "            if not G.has_edge(idx, j):\n",
    "                negs.append([idx, j])\n",
    "    negs = np.array(negs)\n",
    "    e = negs[np.random.permutation(negs.shape[0])[:G.number_of_edges()]]\n",
    "    return np.concatenate([e, np.zeros((e.shape[0], 1))], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = {}\n",
    "if not os.path.exists('new_results'):\n",
    "    os.mkdir('new_results')\n",
    "for d in ['hse']:\n",
    "    print(d)\n",
    "    data = DataWrapper(\"../HSE Data Extracting/Data/edgelist_train.txt\", \"../HSE Data Extracting/Data/edgelist__2015nw.txt\")\n",
    "    features = torch.FloatTensor(np.ones((data.features.shape[0], 1)))\n",
    "    num_feats = features.shape[1]\n",
    "    n_edges = data.graph.number_of_edges()\n",
    "    g = data.graph\n",
    "    # add self loop\n",
    "    g.remove_edges_from(g.selfloop_edges())\n",
    "    g = DGLGraph(g)\n",
    "    n_edges = g.number_of_edges()\n",
    "    res[d] = {}\n",
    "    task = 'lp'\n",
    "    res[d][task] = {}\n",
    "    \n",
    "    degs = g.in_degrees().float()\n",
    "    norm = torch.pow(degs, -0.5)\n",
    "    norm[torch.isinf(norm)] = 0\n",
    "    g.ndata['norm'] = norm.unsqueeze(1)\n",
    "    adj = g.adjacency_matrix().to_dense()\n",
    "    pos_weight = torch.Tensor([float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()])\n",
    "    \n",
    "    edgelist, labels, train_mask, val_mask, test_mask = create_lp_dataset(data.graph)\n",
    "    \n",
    "    edgelist = torch.LongTensor(edgelist)\n",
    "    labels = torch.ByteTensor(labels)\n",
    "    train_mask = torch.LongTensor(train_mask)\n",
    "    val_mask = torch.LongTensor(val_mask)\n",
    "    test_mask = torch.LongTensor(test_mask)\n",
    "    n_classes = 64\n",
    "    loss_fcn = unsup_loss\n",
    "    \n",
    "    for m in ['gcn']:\n",
    "        print(m)\n",
    "        model, args = init_model(g, m, num_feats, n_classes)\n",
    "        stopper = EarlyStopping(patience=100, exp_name=task + '_' + m)\n",
    "        decoder = InnerProductDecoder(activation=lambda x: x)\n",
    "        optimizer = torch.optim.Adam(list(model.parameters()) + list(decoder.parameters()), lr=args.lr, weight_decay=args.weight_decay)\n",
    "        dur = []\n",
    "        for epoch in range(args.epochs):\n",
    "            model.train()\n",
    "            if epoch >= 3:\n",
    "                t0 = time.time()\n",
    "            # forward\n",
    "            logits = model(features)\n",
    "            \n",
    "            preds = decoder(logits)\n",
    "            \n",
    "            loss = F.binary_cross_entropy_with_logits(preds, adj, pos_weight=pos_weight)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if epoch >= 3:\n",
    "                dur.append(time.time() - t0)\n",
    "\n",
    "            train_acc = evaluate(model, features, labels, edgelist, train_mask,  task)\n",
    "\n",
    "            val_acc = evaluate(model, features, labels, edgelist, val_mask, task)\n",
    "            if stopper.step(-loss.item(), model):   \n",
    "                break\n",
    "\n",
    "            if (epoch % 10) == 0:\n",
    "                print(\"Epoch {:05d} | Time(s) {:.4f} | Loss {:.4f} | TrainAcc {:.4f} |\"\n",
    "                      \" ValAcc {:.4f} | ETputs(KTEPS) {:.2f}\".\n",
    "                      format(epoch, np.mean(dur), loss.item(), train_acc,\n",
    "                             val_acc, n_edges / np.mean(dur) / 1000))\n",
    "\n",
    "        print()\n",
    "        model.load_state_dict(torch.load(task + '_' + m + '_es_checkpoint.pt'))\n",
    "        acc = evaluate(model, features, labels, edgelist, test_mask, task)\n",
    "        res[d][task][m] = acc\n",
    "        embeddings = model(features).data.numpy()\n",
    "        pd.DataFrame(embeddings).to_csv(f'{m}_{d}_wo_feats.csv', index=False)\n",
    "        print(\"Test Accuracy {:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.labels.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "invmap = {j: i for i, j in data.mapping.items()}\n",
    "emb_dict_default = dict(zip([invmap[i] for i in range(len(invmap))], embeddings))\n",
    "with open('emb_dict_train_gcn_unsup.pkl', 'wb') as f:\n",
    "    pickle.dump(emb_dict_default, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
