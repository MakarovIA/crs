{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_logistic(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # Logistic regression\n",
    "    logistic_model = LogisticRegression()\n",
    "    logistic_model.fit(X_train, y_train)\n",
    "                     \n",
    "    y_pred_train = logistic_model.predict_proba(X_train)\n",
    "    y_pred_test = logistic_model.predict_proba(X_test)\n",
    "    \n",
    "    precision_train = precision_score(y_train, y_pred_train)\n",
    "    precision_test = precision_score(y_test, y_pred_test)\n",
    "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "    f1_macro_train = f1_score(y_train, y_pred_train, average='macro')\n",
    "    f1_macro_test = f1_score(y_test, y_pred_test, average='macro')\n",
    "    f1_micro_train = f1_score(y_train, y_pred_train, average='micro')\n",
    "    f1_micro_test = f1_score(y_test, y_pred_test, average='micro')\n",
    "    logloss_train = log_loss(y_train, y_pred_train)\n",
    "    logloss_test = log_loss(y_test, y_pred_test)\n",
    "    roc_auc_train = roc_auc_score(y_train, y_pred_train)\n",
    "    roc_auc_test = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "    return precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train, f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train, roc_auc_test, y_pred_train, y_pred_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_svm(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # SVC\n",
    "    svm_model = svm.SVC()\n",
    "    svm_model.fit(X_train, y_train)\n",
    "                     \n",
    "    y_pred_train = svm_model.predict_proba(X_train)\n",
    "    y_pred_test = svm_model.predict_proba(X_test)\n",
    "    \n",
    "    precision_train = precision_score(y_train, y_pred_train)\n",
    "    precision_test = precision_score(y_test, y_pred_test)\n",
    "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "    f1_macro_train = f1_score(y_train, y_pred_train, average='macro')\n",
    "    f1_macro_test = f1_score(y_test, y_pred_test, average='macro')\n",
    "    f1_micro_train = f1_score(y_train, y_pred_train, average='micro')\n",
    "    f1_micro_test = f1_score(y_test, y_pred_test, average='micro')\n",
    "    logloss_train = log_loss(y_train, y_pred_train)\n",
    "    logloss_test = log_loss(y_test, y_pred_test)\n",
    "    roc_auc_train = roc_auc_score(y_train, y_pred_train)\n",
    "    roc_auc_test = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "    return precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train, f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train, roc_auc_test, y_pred_train, y_pred_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_rf(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # RandomForest\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X_train, y_train)\n",
    "                     \n",
    "    y_pred_train = rf.predict_proba(X_train)\n",
    "    y_pred_test = rf.predict_proba(X_test)\n",
    "    \n",
    "    precision_train = precision_score(y_train, y_pred_train)\n",
    "    precision_test = precision_score(y_test, y_pred_test)\n",
    "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "    f1_macro_train = f1_score(y_train, y_pred_train, average='macro')\n",
    "    f1_macro_test = f1_score(y_test, y_pred_test, average='macro')\n",
    "    f1_micro_train = f1_score(y_train, y_pred_train, average='micro')\n",
    "    f1_micro_test = f1_score(y_test, y_pred_test, average='micro')\n",
    "    logloss_train = log_loss(y_train, y_pred_train)\n",
    "    logloss_test = log_loss(y_test, y_pred_test)\n",
    "    roc_auc_train = roc_auc_score(y_train, y_pred_train)\n",
    "    roc_auc_test = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "    return precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train, f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train, roc_auc_test, y_pred_train, y_pred_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_gbc(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # GradientBoosting\n",
    "    params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,\n",
    "              'learning_rate': 0.01, 'loss': 'ls'}\n",
    "    clf = GradientBoostingClassifier(**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "                     \n",
    "    y_pred_train = clf.predict_proba(X_train)\n",
    "    y_pred_test = clf.predict_proba(X_test)\n",
    "    \n",
    "    precision_train = precision_score(y_train, y_pred_train)\n",
    "    precision_test = precision_score(y_test, y_pred_test)\n",
    "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "    f1_macro_train = f1_score(y_train, y_pred_train, average='macro')\n",
    "    f1_macro_test = f1_score(y_test, y_pred_test, average='macro')\n",
    "    f1_micro_train = f1_score(y_train, y_pred_train, average='micro')\n",
    "    f1_micro_test = f1_score(y_test, y_pred_test, average='micro')\n",
    "    logloss_train = log_loss(y_train, y_pred_train)\n",
    "    logloss_test = log_loss(y_test, y_pred_test)\n",
    "    roc_auc_train = roc_auc_score(y_train, y_pred_train)\n",
    "    roc_auc_test = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "    return precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train, f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train, roc_auc_test, y_pred_train, y_pred_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Calculating embeddings (finding best parameters: 'p' and 'q')\n",
    "# Source: https://github.com/aditya-grover/node2vec\n",
    "values = [0.25,0.5,1,2,4] # d=64\n",
    "for x in values:\n",
    "    for y in values:\n",
    "        %run ./node2vec-master/src/main.py --input ./Data/I_scopus_train.txt --output ./Data/emb/scopus/out{x}_{y}.txt --q {x} --p {y} --weighted\n",
    "        %run ./node2vec-master/src/main.py --input ./Data/I_scopus_2017.txt --output ./Data/emb/scopus/out_x{x}_{y}.txt --q {x} --p {y} --weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finding the best 'd' parameter for learning (with p=0.5 and q=1)\n",
    "values_d = [16,32,64,128,256]\n",
    "for d in values_d:\n",
    "    %run ./node2vec-master/src/main.py --input ./Data/I_scopus_train.txt --output ./Data/emb/scopus/out_d_{d}.txt --dimensions {d} --q 1 --p 0.5 --weighted\n",
    "    %run ./node2vec-master/src/main.py --input ./Data/I_scopus_2017.txt --output ./Data/emb/scopus/out_x_d_{d}.txt --dimensions {d} --q 1 --p 0.5 --weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Calculating embeddings for key-words co-occurrence network (finding best parameters: 'p' and 'q')\n",
    "values = [0.25,0.5,1,2,4] # d=64\n",
    "for x in values:\n",
    "    for y in values:\n",
    "        %run ./node2vec-master/src/main.py --input ./Data/I_scopus_keywords_train.txt --output ./Data/emb/scopus_keywords/out{x}_{y}.txt --q {x} --p {y} --weighted\n",
    "        %run ./node2vec-master/src/main.py --input ./Data/I_scopus_keywords_2017.txt --output ./Data/emb/scopus_keywords/out_x{x}_{y}.txt --q {x} --p {y} --weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finding the best 'd' parameter for learning (with p=0.25 and q=1)\n",
    "values_d = [16,32,64,128,256]\n",
    "for d in values_d:\n",
    "    %run ./node2vec-master/src/main.py --input ./Data/I_scopus_keywords_train.txt --output ./Data/emb/scopus_keywords/out_d_{d}.txt --dimensions {d} --q 1 --p 0.25 --weighted\n",
    "    %run ./node2vec-master/src/main.py --input ./Data/I_scopus_keywords_2017.txt --output ./Data/emb/scopus_keywords/out_x_d_{d}.txt --dimensions {d} --q 1 --p 0.25 --weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define edge embbeding functions\n",
    "def avg_sum(v1, v2):\n",
    "    return (np.array(v1)+np.array(v2))/2\n",
    "\n",
    "def mult(v1, v2):\n",
    "    return np.array(v1)*np.array(v2)\n",
    "\n",
    "def w_l1(v1, v2):\n",
    "    return np.abs(np.array(v1)-np.array(v2))\n",
    "\n",
    "def w_l2(v1, v2):\n",
    "    return (np.array(v1)-np.array(v2))**2\n",
    "\n",
    "def nw_l1(v1, v2, graph, n1, n2, embs ):\n",
    "    neig1 = [n for n in graph.neighbors(n1)]\n",
    "    neig2 = [n for n in graph.neighbors(n2)]\n",
    "    sum1 = np.zeros(len(v1))\n",
    "    sum2 = np.zeros(len(v2))\n",
    "    for n in neig1:\n",
    "        sum1 += np.array(embs[int(n)])\n",
    "    for n in neig2:\n",
    "        sum2 += np.array(embs[int(n)])\n",
    "    return np.abs((sum1+np.array(v1))/(len(neig1)+1)-(sum2+np.array(v2))/(len(neig2)+1))\n",
    "\n",
    "def nw_l2(v1, v2, graph, n1, n2, embs ):\n",
    "    neig1 = [n for n in graph.neighbors(n1)]\n",
    "    neig2 = [n for n in graph.neighbors(n2)]\n",
    "    sum1 = np.zeros(len(v1))\n",
    "    sum2 = np.zeros(len(v2))\n",
    "    for n in neig1:\n",
    "        sum1 += np.array(embs[int(n)])\n",
    "    for n in neig2:\n",
    "        sum2 += np.array(embs[int(n)])\n",
    "    return ((sum1+np.array(v1))/(len(neig1)+1)-(sum2+np.array(v2))/(len(neig2)+1))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Loading data ##\n",
    "Graph_train = nx.read_weighted_edgelist(\"./Data/scopus/I_scopus_train.txt\", delimiter=' ',nodetype=int)\n",
    "Graph_2017 = nx.read_weighted_edgelist(\"./Data/scopus/I_scopus_2017.txt\", delimiter=' ',nodetype=int)\n",
    "f_train = open('./Data/scopus/I_scopus_train.txt')\n",
    "f_2017 = open('./Data/scopus/I_scopus_2017.txt')\n",
    "edges_train = []\n",
    "edges_2017 = []\n",
    "for line in f_train:\n",
    "    edges_train.append(line.split(' '))\n",
    "for line in f_2017:\n",
    "    edges_2017.append(line.split(' '))\n",
    "f_train.close()\n",
    "f_2017.close()\n",
    "for i in range(len(edges_train)):\n",
    "    edges_train[i][2] = edges_train[i][2][:-1]\n",
    "for i in range(len(edges_2017)):\n",
    "    edges_2017[i][2] = edges_2017[i][2][:-1]\n",
    "f_emb_train = open('./Data/emb/scopus/out_d_'+str(32)+'.txt')\n",
    "f_emb_2017 = open('./Data/emb/scopus/out_x_d_'+str(32)+'.txt')\n",
    "emb_train = []\n",
    "emb_2017 = []\n",
    "for line in f_emb_train:\n",
    "    emb_train.append(line.split(' '))\n",
    "for line in f_emb_2017:\n",
    "    emb_2017.append(line.split(' '))\n",
    "f_emb_train.close()\n",
    "f_emb_2017.close()\n",
    "for i in range(len(emb_2017)):\n",
    "    emb_2017[i][len(emb_2017[i])-1] = emb_2017[i][len(emb_2017[i])-1][:-1]\n",
    "emb_2017_dict = {} \n",
    "for i in range(1,len(emb_2017)):\n",
    "    emb_2017_dict[int(emb_2017[i][0])] = [float(j) for j in emb_2017[i][1:]]\n",
    "for i in range(len(emb_train)):\n",
    "    emb_train[i][len(emb_train[i])-1] = emb_train[i][len(emb_train[i])-1][:-1]\n",
    "emb_train_dict = {} \n",
    "for i in range(1,len(emb_train)):\n",
    "    emb_train_dict[int(emb_train[i][0])] = [float(j) for j in emb_train[i][1:]]\n",
    "f_emb_train_keywords = open('./Data/emb/scopus_keywords/out_d_'+str(16)+'.txt')\n",
    "f_emb_2017_keywords = open('./Data/emb/scopus_keywords/out_x_d_'+str(16)+'.txt')\n",
    "emb_train_keywords = []\n",
    "emb_2017_keywords = []\n",
    "for line in f_emb_train_keywords:\n",
    "    emb_train_keywords.append(line.split(' '))\n",
    "for line in f_emb_2017_keywords:\n",
    "    emb_2017_keywords.append(line.split(' '))\n",
    "f_emb_train_keywords.close()\n",
    "f_emb_2017_keywords.close()\n",
    "for i in range(len(emb_2017_keywords)):\n",
    "    emb_2017_keywords[i][len(emb_2017_keywords[i])-1] = emb_2017_keywords[i][len(emb_2017_keywords[i])-1][:-1]\n",
    "emb_2017_keywords_dict = {} \n",
    "for i in range(1,len(emb_2017_keywords)):\n",
    "    emb_2017_keywords_dict[int(emb_2017_keywords[i][0])] = [float(j) for j in emb_2017_keywords[i][1:]]\n",
    "for i in range(len(emb_train_keywords)):\n",
    "    emb_train_keywords[i][len(emb_train_keywords[i])-1] = emb_train_keywords[i][len(emb_train_keywords[i])-1][:-1]\n",
    "emb_train_keywords_dict = {} \n",
    "for i in range(1,len(emb_train_keywords)):\n",
    "    emb_train_keywords_dict[int(emb_train_keywords[i][0])] = [float(j) for j in emb_train_keywords[i][1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## 1 ##\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "for e in edges_2017:\n",
    "    emb1 = emb_2017_dict[int(e[0])]\n",
    "    emb2 = emb_2017_dict[int(e[1])]\n",
    "    w = round(float(e[2]))\n",
    "    y_test.append(w)\n",
    "    res = nw_l2(emb1, emb2, Graph_2017,int(e[0]),int(e[1]),emb_2017_dict)\n",
    "    X_test.append(res)\n",
    "for e in edges_train:\n",
    "    emb1 = emb_train_dict[int(e[0])]\n",
    "    emb2 = emb_train_dict[int(e[1])]\n",
    "    w = round(float(e[2]))\n",
    "    y_train.append(w)\n",
    "    res = nw_l2(emb1, emb2, Graph_train,int(e[0]),int(e[1]),emb_train_dict)\n",
    "    X_train.append(res)\n",
    "precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train, f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train, roc_auc_test, y_pred_train, y_pred_test  = model_svm(X_train, y_train, X_test, y_test)\n",
    "print('Precision: ' + str(precision_train))\n",
    "print('Accuracy: ' + str(accuracy_train))\n",
    "print('F-1 (macro): ' + str(f1_macro_train))\n",
    "print('F-1 (micro): ' + str(f1_micro_train))\n",
    "print('Logloss: ' + str(logloss_train))\n",
    "print('ROC-AUC: ' + str(roc_auc_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## 3 ##\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "for e in edges_2017:\n",
    "    pref_att = len(list(nx.common_neighbors(Graph_2017, int(e[0]),int(e[1]))))\n",
    "    jac = list(nx.jaccard_coefficient(Graph_2017, [(int(e[0]),int(e[1]))]))[0][2]\n",
    "    aa = list(nx.adamic_adar_index(Graph_2017, [(int(e[0]),int(e[1]))]))[0][2]\n",
    "    w = round(float(e[2]))\n",
    "    y_test.append(w)\n",
    "    res = np.array([pref_att, jac, aa])\n",
    "    X_test.append(res)\n",
    "for e in edges_train:\n",
    "    pref_att = len(list(nx.common_neighbors(Graph_train, int(e[0]),int(e[1]))))\n",
    "    jac = list(nx.jaccard_coefficient(Graph_train, [(int(e[0]),int(e[1]))]))[0][2]\n",
    "    aa = list(nx.adamic_adar_index(Graph_train, [(int(e[0]),int(e[1]))]))[0][2]\n",
    "    w = round(float(e[2]))\n",
    "    y_train.append(w)\n",
    "    res = np.array([pref_att, jac, aa])\n",
    "    X_train.append(res)\n",
    "precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train, f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train, roc_auc_test, y_pred_train, y_pred_test  = model_svm(X_train, y_train, X_test, y_test)\n",
    "print('Precision: ' + str(precision_train))\n",
    "print('Accuracy: ' + str(accuracy_train))\n",
    "print('F-1 (macro): ' + str(f1_macro_train))\n",
    "print('F-1 (micro): ' + str(f1_micro_train))\n",
    "print('Logloss: ' + str(logloss_train))\n",
    "print('ROC-AUC: ' + str(roc_auc_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## 1 + 2 ##\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "for e in edges_2017:\n",
    "    emb1 = emb_2017_dict[int(e[0])]\n",
    "    emb2 = emb_2017_dict[int(e[1])]\n",
    "    emb1_keywords = emb_2017_keywords_dict[int(e[0])]\n",
    "    emb2_keywords = emb_2017_keywords_dict[int(e[1])]\n",
    "    w = round(float(e[2]))\n",
    "    y_test.append(w)\n",
    "    res = nw_l2(emb1, emb2, Graph_2017,int(e[0]),int(e[1]),emb_2017_dict)\n",
    "    res_keywords = nw_l2(emb1_keywords, emb2_keywords, Graph_2017,int(e[0]),int(e[1]),emb_2017_keywords_dict)\n",
    "    X_test.append(np.concatenate((res, res_keywords)))\n",
    "for e in edges_train:\n",
    "    emb1 = emb_train_dict[int(e[0])]\n",
    "    emb2 = emb_train_dict[int(e[1])]\n",
    "    emb1_keywords = emb_train_keywords_dict[int(e[0])]\n",
    "    emb2_keywords = emb_train_keywords_dict[int(e[1])]\n",
    "    w = round(float(e[2]))\n",
    "    y_train.append(w)\n",
    "    res = nw_l2(emb1, emb2, Graph_train,int(e[0]),int(e[1]),emb_train_dict)\n",
    "    res_keywords = nw_l2(emb1_keywords, emb2_keywords, Graph_train,int(e[0]),int(e[1]),emb_train_keywords_dict)\n",
    "    X_train.append(np.concatenate((res, res_keywords)))\n",
    "precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train, f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train, roc_auc_test, y_pred_train, y_pred_test  = model_svm(X_train, y_train, X_test, y_test)\n",
    "print('Precision: ' + str(precision_train))\n",
    "print('Accuracy: ' + str(accuracy_train))\n",
    "print('F-1 (macro): ' + str(f1_macro_train))\n",
    "print('F-1 (micro): ' + str(f1_micro_train))\n",
    "print('Logloss: ' + str(logloss_train))\n",
    "print('ROC-AUC: ' + str(roc_auc_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## 3 + 4 ##\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "for e in edges_2017:\n",
    "    pref_att = len(list(nx.common_neighbors(Graph_2017, int(e[0]),int(e[1]))))\n",
    "    jac = list(nx.jaccard_coefficient(Graph_2017, [(int(e[0]),int(e[1]))]))[0][2]\n",
    "    aa = list(nx.adamic_adar_index(Graph_2017, [(int(e[0]),int(e[1]))]))[0][2]\n",
    "    cl_coef = nx.clustering(Graph_2017, int(e[0])) + nx.clustering(Graph_2017, int(e[1]))\n",
    "    betw = betw_2017.get(int(e[0])) + betw_2017.get(int(e[0]))\n",
    "    clos = clos_2017.get(int(e[0])) + clos_2017.get(int(e[0]))\n",
    "    sp = nx.shortest_path_length(Graph_2017, int(e[0]),int(e[1]))\n",
    "    w = float(e[2])\n",
    "    y_test.append(w)\n",
    "    res = np.array([pref_att, jac, aa, cl_coef, betw, clos, sp])\n",
    "    X_test.append(res)\n",
    "for e in edges_train:\n",
    "    pref_att = len(list(nx.common_neighbors(Graph_train, int(e[0]),int(e[1]))))\n",
    "    jac = list(nx.jaccard_coefficient(Graph_train, [(int(e[0]),int(e[1]))]))[0][2]\n",
    "    aa = list(nx.adamic_adar_index(Graph_train, [(int(e[0]),int(e[1]))]))[0][2]\n",
    "    cl_coef = nx.clustering(Graph_train, int(e[0])) + nx.clustering(Graph_train, int(e[1]))\n",
    "    betw = betw_train.get(int(e[0])) + betw_train.get(int(e[0]))\n",
    "    clos = clos_train.get(int(e[0])) + clos_train.get(int(e[0]))\n",
    "    sp = nx.shortest_path_length(Graph_train, int(e[0]),int(e[1]))\n",
    "    w = float(e[2])\n",
    "    y_train.append(w)\n",
    "    res = np.array([pref_att, jac, aa, cl_coef, betw, clos, sp])\n",
    "    X_train.append(res)\n",
    "precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train, f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train, roc_auc_test, y_pred_train, y_pred_test  = model_svm(X_train, y_train, X_test, y_test)\n",
    "print('Precision: ' + str(precision_train))\n",
    "print('Accuracy: ' + str(accuracy_train))\n",
    "print('F-1 (macro): ' + str(f1_macro_train))\n",
    "print('F-1 (micro): ' + str(f1_micro_train))\n",
    "print('Logloss: ' + str(logloss_train))\n",
    "print('ROC-AUC: ' + str(roc_auc_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## 1 + 4 ##\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "for e in edges_2017:\n",
    "    cl_coef = nx.clustering(Graph_2017, int(e[0])) + nx.clustering(Graph_2017, int(e[1]))\n",
    "    betw = betw_2017.get(int(e[0])) + betw_2017.get(int(e[0]))\n",
    "    clos = clos_2017.get(int(e[0])) + clos_2017.get(int(e[0]))\n",
    "    sp = nx.shortest_path_length(Graph_2017, int(e[0]),int(e[1]))\n",
    "    emb1 = emb_2017_dict[int(e[0])]\n",
    "    emb2 = emb_2017_dict[int(e[1])]\n",
    "    nw = nw_l2(emb1, emb2, Graph_2017,int(e[0]),int(e[1]),emb_2017_dict)\n",
    "    w = float(e[2])\n",
    "    y_test.append(w)\n",
    "    res = np.concatenate((np.array([cl_coef, betw, clos, sp]), nw))\n",
    "    X_test.append(res)\n",
    "for e in edges_train:\n",
    "    cl_coef = nx.clustering(Graph_train, int(e[0])) + nx.clustering(Graph_train, int(e[1]))\n",
    "    betw = betw_train.get(int(e[0])) + betw_train.get(int(e[0]))\n",
    "    clos = clos_train.get(int(e[0])) + clos_train.get(int(e[0]))\n",
    "    sp = nx.shortest_path_length(Graph_train, int(e[0]),int(e[1]))\n",
    "    emb1 = emb_train_dict[int(e[0])]\n",
    "    emb2 = emb_train_dict[int(e[1])]\n",
    "    nw = nw_l2(emb1, emb2, Graph_train,int(e[0]),int(e[1]),emb_train_dict)\n",
    "    w = float(e[2])\n",
    "    y_train.append(w)\n",
    "    res = np.concatenate((np.array([cl_coef, betw, clos, sp]), nw))\n",
    "    X_train.append(res)\n",
    "precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train, f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train, roc_auc_test, y_pred_train, y_pred_test  = model_svm(X_train, y_train, X_test, y_test)\n",
    "print('Precision: ' + str(precision_train))\n",
    "print('Accuracy: ' + str(accuracy_train))\n",
    "print('F-1 (macro): ' + str(f1_macro_train))\n",
    "print('F-1 (micro): ' + str(f1_micro_train))\n",
    "print('Logloss: ' + str(logloss_train))\n",
    "print('ROC-AUC: ' + str(roc_auc_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## 1 + 3 ##\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "for e in edges_2017:\n",
    "    pref_att = len(list(nx.common_neighbors(Graph_2017, int(e[0]),int(e[1]))))\n",
    "    jac = list(nx.jaccard_coefficient(Graph_2017, [(int(e[0]),int(e[1]))]))[0][2]\n",
    "    aa = list(nx.adamic_adar_index(Graph_2017, [(int(e[0]),int(e[1]))]))[0][2]\n",
    "    emb1 = emb_2017_dict[int(e[0])]\n",
    "    emb2 = emb_2017_dict[int(e[1])]\n",
    "    nw = nw_l2(emb1, emb2, Graph_2017,int(e[0]),int(e[1]),emb_2017_dict)\n",
    "    w = float(e[2])\n",
    "    y_test.append(w)\n",
    "    res = np.concatenate((np.array([pref_att, jac, aa]), nw))\n",
    "    X_test.append(res)\n",
    "for e in edges_train:\n",
    "    pref_att = len(list(nx.common_neighbors(Graph_train, int(e[0]),int(e[1]))))\n",
    "    jac = list(nx.jaccard_coefficient(Graph_train, [(int(e[0]),int(e[1]))]))[0][2]\n",
    "    aa = list(nx.adamic_adar_index(Graph_train, [(int(e[0]),int(e[1]))]))[0][2]\n",
    "    emb1 = emb_train_dict[int(e[0])]\n",
    "    emb2 = emb_train_dict[int(e[1])]\n",
    "    nw = nw_l2(emb1, emb2, Graph_train,int(e[0]),int(e[1]),emb_train_dict)\n",
    "    w = float(e[2])\n",
    "    y_train.append(w)\n",
    "    res = np.concatenate((np.array([pref_att, jac, aa]), nw))\n",
    "    X_train.append(res)\n",
    "precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train, f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train, roc_auc_test, y_pred_train, y_pred_test  = model_svm(X_train, y_train, X_test, y_test)\n",
    "print('Precision: ' + str(precision_train))\n",
    "print('Accuracy: ' + str(accuracy_train))\n",
    "print('F-1 (macro): ' + str(f1_macro_train))\n",
    "print('F-1 (micro): ' + str(f1_micro_train))\n",
    "print('Logloss: ' + str(logloss_train))\n",
    "print('ROC-AUC: ' + str(roc_auc_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## 1 + 2 + 3 ##\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "for e in edges_2017:\n",
    "    pref_att = len(list(nx.common_neighbors(Graph_2017, int(e[0]),int(e[1]))))\n",
    "    jac = list(nx.jaccard_coefficient(Graph_2017, [(int(e[0]),int(e[1]))]))[0][2]\n",
    "    aa = list(nx.adamic_adar_index(Graph_2017, [(int(e[0]),int(e[1]))]))[0][2]\n",
    "    emb1 = emb_2017_dict[int(e[0])]\n",
    "    emb2 = emb_2017_dict[int(e[1])]\n",
    "    emb1_keywords = emb_2017_keywords_dict[int(e[0])]\n",
    "    emb2_keywords = emb_2017_keywords_dict[int(e[1])]\n",
    "    nw = nw_l2(emb1, emb2, Graph_2017,int(e[0]),int(e[1]),emb_2017_dict)\n",
    "    nw_keywords = nw_l2(emb1_keywords, emb2_keywords, Graph_2017,int(e[0]),int(e[1]),emb_2017_keywords_dict)\n",
    "    w = float(e[2])\n",
    "    y_test.append(w)\n",
    "    res = np.concatenate((np.array([pref_att, jac, aa]), nw))\n",
    "    X_test.append(np.concatenate((res, nw_keywords)))\n",
    "for e in edges_train:\n",
    "    pref_att = len(list(nx.common_neighbors(Graph_train, int(e[0]),int(e[1]))))\n",
    "    jac = list(nx.jaccard_coefficient(Graph_train, [(int(e[0]),int(e[1]))]))[0][2]\n",
    "    aa = list(nx.adamic_adar_index(Graph_train, [(int(e[0]),int(e[1]))]))[0][2]\n",
    "    emb1 = emb_train_dict[int(e[0])]\n",
    "    emb2 = emb_train_dict[int(e[1])]\n",
    "    emb1_keywords = emb_train_keywords_dict[int(e[0])]\n",
    "    emb2_keywords = emb_train_keywords_dict[int(e[1])]\n",
    "    w = round(float(e[2]))\n",
    "    y_train.append(w)\n",
    "    nw = nw_l2(emb1, emb2, Graph_train,int(e[0]),int(e[1]),emb_train_dict)\n",
    "    nw_keywords = nw_l2(emb1_keywords, emb2_keywords, Graph_train,int(e[0]),int(e[1]),emb_train_keywords_dict)\n",
    "    res = np.concatenate((np.array([pref_att, jac, aa]), nw))\n",
    "    X_train.append(np.concatenate((res, nw_keywords)))\n",
    "precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train, f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train, roc_auc_test, y_pred_train, y_pred_test  = model_svm(X_train, y_train, X_test, y_test)\n",
    "print('Precision: ' + str(precision_train))\n",
    "print('Accuracy: ' + str(accuracy_train))\n",
    "print('F-1 (macro): ' + str(f1_macro_train))\n",
    "print('F-1 (micro): ' + str(f1_micro_train))\n",
    "print('Logloss: ' + str(logloss_train))\n",
    "print('ROC-AUC: ' + str(roc_auc_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## 1 + 2 + 4 ##\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "for e in edges_2017:\n",
    "    cl_coef = nx.clustering(Graph_2017, int(e[0])) + nx.clustering(Graph_2017, int(e[1]))\n",
    "    betw = betw_2017.get(int(e[0])) + betw_2017.get(int(e[0]))\n",
    "    clos = clos_2017.get(int(e[0])) + clos_2017.get(int(e[0]))\n",
    "    sp = nx.shortest_path_length(Graph_2017, int(e[0]),int(e[1]))\n",
    "    emb1 = emb_2017_dict[int(e[0])]\n",
    "    emb2 = emb_2017_dict[int(e[1])]\n",
    "    emb1_keywords = emb_2017_keywords_dict[int(e[0])]\n",
    "    emb2_keywords = emb_2017_keywords_dict[int(e[1])]\n",
    "    nw = nw_l2(emb1, emb2, Graph_2017,int(e[0]),int(e[1]),emb_2017_dict)\n",
    "    nw_keywords = nw_l2(emb1_keywords, emb2_keywords, Graph_2017,int(e[0]),int(e[1]),emb_2017_keywords_dict)\n",
    "    w = float(e[2])\n",
    "    y_test.append(w)\n",
    "    res = np.concatenate((np.array([cl_coef, betw, clos, sp]), nw))\n",
    "    X_test.append(np.concatenate((res, nw_keywords)))\n",
    "for e in edges_train:\n",
    "    cl_coef = nx.clustering(Graph_train, int(e[0])) + nx.clustering(Graph_train, int(e[1]))\n",
    "    betw = betw_train.get(int(e[0])) + betw_train.get(int(e[0]))\n",
    "    clos = clos_train.get(int(e[0])) + clos_train.get(int(e[0]))\n",
    "    sp = nx.shortest_path_length(Graph_train, int(e[0]),int(e[1]))\n",
    "    emb1 = emb_train_dict[int(e[0])]\n",
    "    emb2 = emb_train_dict[int(e[1])]\n",
    "    emb1_keywords = emb_train_keywords_dict[int(e[0])]\n",
    "    emb2_keywords = emb_train_keywords_dict[int(e[1])]\n",
    "    w = round(float(e[2]))\n",
    "    y_train.append(w)\n",
    "    nw = nw_l2(emb1, emb2, Graph_train,int(e[0]),int(e[1]),emb_train_dict)\n",
    "    nw_keywords = nw_l2(emb1_keywords, emb2_keywords, Graph_train,int(e[0]),int(e[1]),emb_train_keywords_dict)\n",
    "    res = np.concatenate((np.array([cl_coef, betw, clos, sp]), nw))\n",
    "    X_train.append(np.concatenate((res, nw_keywords)))\n",
    "precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train, f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train, roc_auc_test, y_pred_train, y_pred_test  = model_svm(X_train, y_train, X_test, y_test)\n",
    "print('Precision: ' + str(precision_train))\n",
    "print('Accuracy: ' + str(accuracy_train))\n",
    "print('F-1 (macro): ' + str(f1_macro_train))\n",
    "print('F-1 (micro): ' + str(f1_micro_train))\n",
    "print('Logloss: ' + str(logloss_train))\n",
    "print('ROC-AUC: ' + str(roc_auc_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## 1 + 2 + 3 + 4 ##\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "for e in edges_2017:\n",
    "    pref_att = len(list(nx.common_neighbors(Graph_2017, int(e[0]),int(e[1]))))\n",
    "    jac = list(nx.jaccard_coefficient(Graph_2017, [(int(e[0]),int(e[1]))]))[0][2]\n",
    "    aa = list(nx.adamic_adar_index(Graph_2017, [(int(e[0]),int(e[1]))]))[0][2]\n",
    "    cl_coef = nx.clustering(Graph_2017, int(e[0])) + nx.clustering(Graph_2017, int(e[1]))\n",
    "    betw = betw_2017.get(int(e[0])) + betw_2017.get(int(e[0]))\n",
    "    clos = clos_2017.get(int(e[0])) + clos_2017.get(int(e[0]))\n",
    "    sp = nx.shortest_path_length(Graph_2017, int(e[0]),int(e[1]))\n",
    "    emb1 = emb_2017_dict[int(e[0])]\n",
    "    emb2 = emb_2017_dict[int(e[1])]\n",
    "    emb1_keywords = emb_2017_keywords_dict[int(e[0])]\n",
    "    emb2_keywords = emb_2017_keywords_dict[int(e[1])]\n",
    "    nw = nw_l2(emb1, emb2, Graph_2017,int(e[0]),int(e[1]),emb_2017_dict)\n",
    "    nw_keywords = nw_l2(emb1_keywords, emb2_keywords, Graph_2017,int(e[0]),int(e[1]),emb_2017_keywords_dict)\n",
    "    w = float(e[2])\n",
    "    y_test.append(w)\n",
    "    res = np.concatenate((np.array([pref_att, jac, aa, cl_coef, betw, clos, sp]), nw))\n",
    "    X_test.append(np.concatenate((res, nw_keywords)))\n",
    "for e in edges_train:\n",
    "    pref_att = len(list(nx.common_neighbors(Graph_train, int(e[0]),int(e[1]))))\n",
    "    jac = list(nx.jaccard_coefficient(Graph_train, [(int(e[0]),int(e[1]))]))[0][2]\n",
    "    aa = list(nx.adamic_adar_index(Graph_train, [(int(e[0]),int(e[1]))]))[0][2]\n",
    "    cl_coef = nx.clustering(Graph_train, int(e[0])) + nx.clustering(Graph_train, int(e[1]))\n",
    "    betw = betw_train.get(int(e[0])) + betw_train.get(int(e[0]))\n",
    "    clos = clos_train.get(int(e[0])) + clos_train.get(int(e[0]))\n",
    "    sp = nx.shortest_path_length(Graph_train, int(e[0]),int(e[1]))\n",
    "    emb1 = emb_train_dict[int(e[0])]\n",
    "    emb2 = emb_train_dict[int(e[1])]\n",
    "    emb1_keywords = emb_train_keywords_dict[int(e[0])]\n",
    "    emb2_keywords = emb_train_keywords_dict[int(e[1])]\n",
    "    w = round(float(e[2]))\n",
    "    y_train.append(w)\n",
    "    nw = nw_l2(emb1, emb2, Graph_train,int(e[0]),int(e[1]),emb_train_dict)\n",
    "    nw_keywords = nw_l2(emb1_keywords, emb2_keywords, Graph_train,int(e[0]),int(e[1]),emb_train_keywords_dict)\n",
    "    res = np.concatenate((np.array([pref_att, jac, aa, cl_coef, betw, clos, sp]), nw))\n",
    "    X_train.append(np.concatenate((res, nw_keywords)))\n",
    "precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train, f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train, roc_auc_test, y_pred_train, y_pred_test  = model_svm(X_train, y_train, X_test, y_test)\n",
    "print('Precision: ' + str(precision_train))\n",
    "print('Accuracy: ' + str(accuracy_train))\n",
    "print('F-1 (macro): ' + str(f1_macro_train))\n",
    "print('F-1 (micro): ' + str(f1_micro_train))\n",
    "print('Logloss: ' + str(logloss_train))\n",
    "print('ROC-AUC: ' + str(roc_auc_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
