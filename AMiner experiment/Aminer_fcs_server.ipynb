{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import collections\n",
    "#import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import math\n",
    "import re\n",
    "#from difflib import SequenceMatcher\n",
    "from tqdm import tqdm, tnrange, tqdm_notebook\n",
    "import mmap\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 11195956107030461792\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 2201773105136009482\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_lines(file_path):\n",
    "    fp = open(file_path, \"r+\")\n",
    "    buf = mmap.mmap(fp.fileno(), 0)\n",
    "    lines = 0\n",
    "    while buf.readline():\n",
    "        lines += 1\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1559889/1559889 [00:30<00:00, 51288.59it/s]\n"
     ]
    }
   ],
   "source": [
    "emb = []\n",
    "with open('word2vec_output.txt', 'r') as f_emb:\n",
    "    for line in tqdm(f_emb, total=get_num_lines('word2vec_output.txt')):\n",
    "        emb.append(line.split(' '))\n",
    "f_emb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1559889/1559889 [00:01<00:00, 1103742.28it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(emb))):\n",
    "    emb[i][len(emb[i])-1] = emb[i][len(emb[i])-1][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1559888/1559888 [00:49<00:00, 31682.10it/s]\n"
     ]
    }
   ],
   "source": [
    "emb_dict = {}\n",
    "for i in tqdm(range(1,len(emb))):\n",
    "    emb_dict[int(emb[i][0])] = [float(j) for j in emb[i][1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4258946/4258946 [00:23<00:00, 184476.44it/s]\n"
     ]
    }
   ],
   "source": [
    "G = nx.Graph() \n",
    "with open('AMiner-Coauthor.txt', 'r') as file:\n",
    "    for line in tqdm(file, total=get_num_lines('AMiner-Coauthor.txt')):\n",
    "        line_arr = line.split()\n",
    "        G.add_edge(int(line_arr[0][1:]), int(line_arr[1]), weight = int(line_arr[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Edges_pos = list(G.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_agj = G.adjacency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_AMiner = np.array(G.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[88064, 593920, 1341443, 1503235, 1417221]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missed = set(nodes_AMiner)-set(emb_dict)\n",
    "list(missed)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_e = G.number_of_edges()\n",
    "num_n = G.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4258946"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1560640"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_pos = []\n",
    "for e in Edges_pos:\n",
    "    if e[0] not in missed and e[1] not in missed:\n",
    "        edges_pos.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_AMiner_neg = []\n",
    "kol_e = 0\n",
    "N = int(math.ceil(num_e/float(num_n)))\n",
    "for i in G_agj:\n",
    "    k = 0\n",
    "    while k != N:\n",
    "        j = random.choice(nodes_AMiner)\n",
    "        if j not in i[1].keys():\n",
    "            edges_AMiner_neg.append((i[0],j))\n",
    "            k += 1\n",
    "            kol_e += 1\n",
    "            if kol_e == num_e:\n",
    "                break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запись\n",
    "Edges_neg = []      \n",
    "fw_neg = open('AMiner-Coauthor_neg2.txt', 'w')                                \n",
    "for e in edges_AMiner_neg:\n",
    "    fw_neg.write(str(e[0])+\" \"+str(e[1])+'\\n')\n",
    "    Edges_neg.append((int(e[0]),int(e[1])))\n",
    "fw_neg.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4681919/4681919 [00:04<00:00, 999423.00it/s] \n"
     ]
    }
   ],
   "source": [
    "# Считывание\n",
    "Edges_neg = []    \n",
    "with open('AMiner-Coauthor_neg2.txt', 'r') as file:\n",
    "    for line in tqdm(file, total=get_num_lines('AMiner-Coauthor_neg2.txt')):\n",
    "        e = line.split()\n",
    "        Edges_neg.append((int(e[0]),int(e[1])))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_neg = []\n",
    "for e in Edges_neg:\n",
    "    if e[0] not in missed and e[1] not in missed:\n",
    "        edges_neg.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4258570"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(edges_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4677469"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(edges_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_svm(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # SVC\n",
    "    svm_model = svm.SVC(probability=True)\n",
    "    svm_model.fit(X_train, y_train)\n",
    "                     \n",
    "    y_pred_train_res = svm_model.predict_proba(X_train)#svm_model.predict_proba(X_train)\n",
    "    y_pred_test_res = svm_model.predict_proba(X_test)#svm_model.predict_proba(X_test)\n",
    "    y_pred_train = [np.argmax(y_pred_train_res[i]) for i in range(len(y_pred_train_res))]\n",
    "    y_pred_test = [np.argmax(y_pred_test_res[i]) for i in range(len(y_pred_test_res))]\n",
    "    \n",
    "    precision_train = precision_score(y_train, y_pred_train)\n",
    "    precision_test = precision_score(y_test, y_pred_test)\n",
    "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "    f1_macro_train = f1_score(y_train, y_pred_train, average='macro')\n",
    "    f1_macro_test = f1_score(y_test, y_pred_test, average='macro')\n",
    "    f1_micro_train = f1_score(y_train, y_pred_train, average='micro')\n",
    "    f1_micro_test = f1_score(y_test, y_pred_test, average='micro')\n",
    "    logloss_train = log_loss(y_train, y_pred_train)\n",
    "    logloss_test = log_loss(y_test, y_pred_test)\n",
    "    roc_auc_train = roc_auc_score(y_train, y_pred_train)\n",
    "    roc_auc_test = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "    return precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train, f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train, roc_auc_test, y_pred_train, y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_logistic(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # Logistic regression\n",
    "    logistic_model = LogisticRegression()\n",
    "    logistic_model.fit(X_train, y_train)\n",
    "                     \n",
    "    y_pred_train_res = logistic_model.predict_proba(X_train)\n",
    "    y_pred_test_res = logistic_model.predict_proba(X_test)\n",
    "    y_pred_train = [np.argmax(y_pred_train_res[i]) for i in range(len(y_pred_train_res))]\n",
    "    y_pred_test = [np.argmax(y_pred_test_res[i]) for i in range(len(y_pred_test_res))]\n",
    "    \n",
    "    precision_train = precision_score(y_train, y_pred_train)\n",
    "    precision_test = precision_score(y_test, y_pred_test)\n",
    "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "    f1_macro_train = f1_score(y_train, y_pred_train, average='macro')\n",
    "    f1_macro_test = f1_score(y_test, y_pred_test, average='macro')\n",
    "    f1_micro_train = f1_score(y_train, y_pred_train, average='micro')\n",
    "    f1_micro_test = f1_score(y_test, y_pred_test, average='micro')\n",
    "    logloss_train = log_loss(y_train, y_pred_train)\n",
    "    logloss_test = log_loss(y_test, y_pred_test)\n",
    "    roc_auc_train = roc_auc_score(y_train, y_pred_train)\n",
    "    roc_auc_test = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "    return precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train, f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train, roc_auc_test, y_pred_train, y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_rf(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # RandomForest\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X_train, y_train)\n",
    "                     \n",
    "    y_pred_train_res = rf.predict_proba(X_train)\n",
    "    y_pred_test_res = rf.predict_proba(X_test)\n",
    "    y_pred_train = [np.argmax(y_pred_train_res[i]) for i in range(len(y_pred_train_res))]\n",
    "    y_pred_test = [np.argmax(y_pred_test_res[i]) for i in range(len(y_pred_test_res))]\n",
    "    \n",
    "    precision_train = precision_score(y_train, y_pred_train)\n",
    "    precision_test = precision_score(y_test, y_pred_test)\n",
    "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "    f1_macro_train = f1_score(y_train, y_pred_train, average='macro')\n",
    "    f1_macro_test = f1_score(y_test, y_pred_test, average='macro')\n",
    "    f1_micro_train = f1_score(y_train, y_pred_train, average='micro')\n",
    "    f1_micro_test = f1_score(y_test, y_pred_test, average='micro')\n",
    "    logloss_train = log_loss(y_train, y_pred_train)\n",
    "    logloss_test = log_loss(y_test, y_pred_test)\n",
    "    roc_auc_train = roc_auc_score(y_train, y_pred_train)\n",
    "    roc_auc_test = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "    return precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train, f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train, roc_auc_test, y_pred_train, y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_gbc(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # Gradient Boosting\n",
    "    params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,\n",
    "              'learning_rate': 0.01, 'loss': 'ls'}\n",
    "    clf = GradientBoostingClassifier(**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "                     \n",
    "    y_pred_train_res = clf.predict_proba(X_train)\n",
    "    y_pred_test_res = clf.predict_proba(X_test)\n",
    "    y_pred_train = [np.argmax(y_pred_train_res[i]) for i in range(len(y_pred_train_res))]\n",
    "    y_pred_test = [np.argmax(y_pred_test_res[i]) for i in range(len(y_pred_test_res))]\n",
    "    \n",
    "    precision_train = precision_score(y_train, y_pred_train)\n",
    "    precision_test = precision_score(y_test, y_pred_test)\n",
    "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "    f1_macro_train = f1_score(y_train, y_pred_train, average='macro')\n",
    "    f1_macro_test = f1_score(y_test, y_pred_test, average='macro')\n",
    "    f1_micro_train = f1_score(y_train, y_pred_train, average='micro')\n",
    "    f1_micro_test = f1_score(y_test, y_pred_test, average='micro')\n",
    "    logloss_train = log_loss(y_train, y_pred_train)\n",
    "    logloss_test = log_loss(y_test, y_pred_test)\n",
    "    roc_auc_train = roc_auc_score(y_train, y_pred_train)\n",
    "    roc_auc_test = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "    return precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train, f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train, roc_auc_test, y_pred_train, y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models(X_train, y_train, X_test, y_test):\n",
    "    # LogReg\n",
    "    precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train,f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train,roc_auc_test, y_pred_train, y_pred_test  = model_logistic(X_train, y_train, X_test, y_test)\n",
    "    print(\"Logistic regression\")\n",
    "    print('Precision: ' + str(precision_train))\n",
    "    print('Accuracy: ' + str(accuracy_train))\n",
    "    print('F-1 (macro): ' + str(f1_macro_train))\n",
    "    print('F-1 (micro): ' + str(f1_micro_train))\n",
    "    print('Logloss: ' + str(logloss_train))\n",
    "    print('ROC-AUC: ' + str(roc_auc_train))\n",
    "    # SVM\n",
    "    precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train, f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train,roc_auc_test, y_pred_train, y_pred_test  = model_svm(X_train, y_train, X_test, y_test)\n",
    "    print(\"SVM\")\n",
    "    print('Precision: ' + str(precision_train))\n",
    "    print('Accuracy: ' + str(accuracy_train))\n",
    "    print('F-1 (macro): ' + str(f1_macro_train))\n",
    "    print('F-1 (micro): ' + str(f1_micro_train))\n",
    "    print('Logloss: ' + str(logloss_train))\n",
    "    print('ROC-AUC: ' + str(roc_auc_train))\n",
    "    # RF\n",
    "    precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train,f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train,roc_auc_test, y_pred_train, y_pred_test  = model_rf(X_train, y_train, X_test, y_test)\n",
    "    print(\"Random Forest\")\n",
    "    print('Precision: ' + str(precision_train))\n",
    "    print('Accuracy: ' + str(accuracy_train))\n",
    "    print('F-1 (macro): ' + str(f1_macro_train))\n",
    "    print('F-1 (micro): ' + str(f1_micro_train))\n",
    "    print('Logloss: ' + str(logloss_train))\n",
    "    print('ROC-AUC: ' + str(roc_auc_train))\n",
    "    # GBC\n",
    "    precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train,f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train,roc_auc_test, y_pred_train, y_pred_test  = model_gbc(X_train, y_train, X_test, y_test)\n",
    "    print(\"Gradient Boosting\")\n",
    "    print('Precision: ' + str(precision_train))\n",
    "    print('Accuracy: ' + str(accuracy_train))\n",
    "    print('F-1 (macro): ' + str(f1_macro_train))\n",
    "    print('F-1 (micro): ' + str(f1_micro_train))\n",
    "    print('Logloss: ' + str(logloss_train))\n",
    "    print('ROC-AUC: ' + str(roc_auc_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define edge embbeding functions\n",
    "def avg_sum(v1, v2):\n",
    "    return (np.array(v1)+np.array(v2))/2\n",
    "\n",
    "def mult(v1, v2):\n",
    "    return np.array(v1)*np.array(v2)\n",
    "\n",
    "def w_l1(v1, v2):\n",
    "    return np.abs(np.array(v1)-np.array(v2))\n",
    "\n",
    "def w_l2(v1, v2):\n",
    "    return (np.array(v1)-np.array(v2))**2\n",
    "\n",
    "def nw_l1(v1, v2, graph, n1, n2, embs ):\n",
    "    #neig1 = [n for n in graph.neighbors(n1)]\n",
    "    #neig2 = [n for n in graph.neighbors(n2)]\n",
    "    sum1 = np.zeros(len(v1))\n",
    "    sum2 = np.zeros(len(v2))\n",
    "    for n in graph.neighbors(n1):\n",
    "        sum1 += np.array(embs[int(n)])\n",
    "    for n in graph.neighbors(n2):\n",
    "        sum2 += np.array(embs[int(n)])\n",
    "    return np.abs((sum1+np.array(v1))/(graph.degree(n1)+1)-(sum2+np.array(v2))/(graph.degree(n2)+1))\n",
    "\n",
    "def nw_l2(v1, v2, graph, n1, n2, embs ):\n",
    "    #neig1 = [n for n in graph.neighbors(n1)]\n",
    "    #neig2 = [n for n in graph.neighbors(n2)]\n",
    "    sum1 = np.zeros(len(v1))\n",
    "    sum2 = np.zeros(len(v2))\n",
    "    for n in graph.neighbors(n1):\n",
    "        sum1 += np.array(embs[int(n)])\n",
    "    for n in graph.neighbors(n2):\n",
    "        sum2 += np.array(embs[int(n)])\n",
    "    return ((sum1+np.array(v1))/(graph.degree(n1)+1)-(sum2+np.array(v2))/(graph.degree(n2)+1))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 38s, sys: 386 ms, total: 1min 38s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## 6: avg_sum ##\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "num_train = int(round(.8*len(edges_pos)))\n",
    "m = 0\n",
    "for e in edges_pos[:num_train]:\n",
    "        emb1 = emb_dict[int(e[0])]\n",
    "        emb2 = emb_dict[int(e[1])]\n",
    "        w = 1 #G_sub20[int(e[0])][int(e[1])]['weight']\n",
    "        y_train.append(w)\n",
    "        res = avg_sum(emb1, emb2)\n",
    "        X_train.append(res)\n",
    "#\n",
    "for e in edges_pos[num_train:]:\n",
    "        emb1 = emb_dict[int(e[0])]\n",
    "        emb2 = emb_dict[int(e[1])]\n",
    "        w = 1 \n",
    "        y_test.append(w)\n",
    "        res = avg_sum(emb1, emb2)\n",
    "        X_test.append(res)\n",
    "#\n",
    "for e in edges_neg[:num_train]:\n",
    "        emb1 = emb_dict[int(e[0])]\n",
    "        emb2 = emb_dict[int(e[1])]\n",
    "        w = 0 \n",
    "        y_train.append(w)\n",
    "        res = avg_sum(emb1, emb2)\n",
    "        X_train.append(res)\n",
    "#\n",
    "for e in edges_neg[num_train:]:\n",
    "        emb1 = emb_dict[int(e[0])]\n",
    "        emb2 = emb_dict[int(e[1])]\n",
    "        w = 0 \n",
    "        y_test.append(w)\n",
    "        res = avg_sum(emb1, emb2)\n",
    "        X_test.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olga3993/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression (avg_sum): test\n",
      "Precision: 0.5141144414168938\n",
      "Accuracy: 0.6040581870748476\n",
      "F-1 (macro): 0.5247652242339096\n",
      "F-1 (micro): 0.6040581870748476\n",
      "Logloss: 13.675419643275355\n",
      "ROC-AUC: 0.5446534292916163\n",
      "Logistic regression (avg_sum): train\n",
      "Precision: 0.7449377504293074\n",
      "Accuracy: 0.737135646472877\n",
      "F-1 (macro): 0.7370689513775414\n",
      "F-1 (micro): 0.737135646472877\n",
      "Logloss: 9.079111854019526\n",
      "ROC-AUC: 0.7371356464728771\n",
      "CPU times: user 2min 41s, sys: 9.07 s, total: 2min 50s\n",
      "Wall time: 2min 43s\n"
     ]
    }
   ],
   "source": [
    "    %%time\n",
    "    # LogReg\n",
    "    precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train,f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train,roc_auc_test, y_pred_train, y_pred_test  = model_logistic(X_train, y_train, X_test, y_test)\n",
    "    print(\"Logistic regression (avg_sum): test\")\n",
    "    print('Precision: ' + str(precision_test))\n",
    "    print('Accuracy: ' + str(accuracy_test))\n",
    "    print('F-1 (macro): ' + str(f1_macro_test))\n",
    "    print('F-1 (micro): ' + str(f1_micro_test))\n",
    "    print('Logloss: ' + str(logloss_test))\n",
    "    print('ROC-AUC: ' + str(roc_auc_test))\n",
    "    print(\"Logistic regression (avg_sum): train\")\n",
    "    print('Precision: ' + str(precision_train))\n",
    "    print('Accuracy: ' + str(accuracy_train))\n",
    "    print('F-1 (macro): ' + str(f1_macro_train))\n",
    "    print('F-1 (micro): ' + str(f1_micro_train))\n",
    "    print('Logloss: ' + str(logloss_train))\n",
    "    print('ROC-AUC: ' + str(roc_auc_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 22s, sys: 352 ms, total: 1min 23s\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## 5: mult ##\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "num_train = int(round(.00001*len(edges_pos)))\n",
    "for e in edges_pos[:num_train]:\n",
    "    emb1 = emb_dict[int(e[0])]\n",
    "    emb2 = emb_dict[int(e[1])]\n",
    "    w = 1 #G_sub20[int(e[0])][int(e[1])]['weight']\n",
    "    y_train.append(w)\n",
    "    res = mult(emb1, emb2)\n",
    "    X_train.append(res)\n",
    "#\n",
    "for e in edges_pos[num_train:]:\n",
    "    emb1 = emb_dict[int(e[0])]\n",
    "    emb2 = emb_dict[int(e[1])]\n",
    "    w = 1 \n",
    "    y_test.append(w)\n",
    "    res = mult(emb1, emb2)\n",
    "    X_test.append(res)\n",
    "#\n",
    "for e in edges_neg[:num_train]:\n",
    "    emb1 = emb_dict[int(e[0])]\n",
    "    emb2 = emb_dict[int(e[1])]\n",
    "    w = 0 \n",
    "    y_train.append(w)\n",
    "    res = mult(emb1, emb2)\n",
    "    X_train.append(res)\n",
    "#\n",
    "for e in edges_neg[num_train:]:\n",
    "    emb1 = emb_dict[int(e[0])]\n",
    "    emb2 = emb_dict[int(e[1])]\n",
    "    w = 0 \n",
    "    y_test.append(w)\n",
    "    res = mult(emb1, emb2)\n",
    "    X_test.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olga3993/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression (mult): test\n",
      "Precision: 0.9960359759584404\n",
      "Accuracy: 0.6519629187843758\n",
      "F-1 (macro): 0.5880497920028142\n",
      "F-1 (micro): 0.6519629187843758\n",
      "Logloss: 12.020775335871873\n",
      "ROC-AUC: 0.6348934713553004\n",
      "Logistic regression (mult): train\n",
      "Precision: 1.0\n",
      "Accuracy: 1.0\n",
      "F-1 (macro): 1.0\n",
      "F-1 (micro): 1.0\n",
      "Logloss: 9.992007221626413e-16\n",
      "ROC-AUC: 1.0\n",
      "CPU times: user 48.6 s, sys: 5.47 s, total: 54.1 s\n",
      "Wall time: 46.8 s\n"
     ]
    }
   ],
   "source": [
    "    %%time\n",
    "    # LogReg\n",
    "    precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train,f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train,roc_auc_test, y_pred_train, y_pred_test  = model_logistic(X_train, y_train, X_test, y_test)\n",
    "    print(\"Logistic regression (mult): test\")\n",
    "    print('Precision: ' + str(precision_test))\n",
    "    print('Accuracy: ' + str(accuracy_test))\n",
    "    print('F-1 (macro): ' + str(f1_macro_test))\n",
    "    print('F-1 (micro): ' + str(f1_micro_test))\n",
    "    print('Logloss: ' + str(logloss_test))\n",
    "    print('ROC-AUC: ' + str(roc_auc_test))\n",
    "    print(\"Logistic regression (mult): train\")\n",
    "    print('Precision: ' + str(precision_train))\n",
    "    print('Accuracy: ' + str(accuracy_train))\n",
    "    print('F-1 (macro): ' + str(f1_macro_train))\n",
    "    print('F-1 (micro): ' + str(f1_micro_train))\n",
    "    print('Logloss: ' + str(logloss_train))\n",
    "    print('ROC-AUC: ' + str(roc_auc_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olga3993/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "    # Logistic regression\n",
    "    logistic_model = LogisticRegression()\n",
    "    logistic_model.fit(X_train, y_train)\n",
    "                     \n",
    "    y_pred_train_res = logistic_model.predict_proba(X_train)\n",
    "    y_pred_test_res = logistic_model.predict_proba(X_test)\n",
    "    y_pred_train = [np.argmax(y_pred_train_res[i]) for i in range(len(y_pred_train_res))]\n",
    "    y_pred_test = [np.argmax(y_pred_test_res[i]) for i in range(len(y_pred_test_res))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0300981 , -0.06513173, -0.319959  ,  0.06301884,  0.1433852 ,\n",
       "        -0.24352267,  0.10440547,  0.35089039,  0.5177037 ,  0.11181678,\n",
       "         0.19231446, -0.02313459, -0.13008327,  0.01908783,  0.57745613,\n",
       "         0.15476395,  0.29231465,  0.01416023, -0.19845081,  0.16711637,\n",
       "        -0.33290153,  0.2176758 ,  0.33928428,  0.0624466 ,  0.48898385,\n",
       "         0.09226164,  0.58650877,  0.12591223, -0.16382232,  0.41285882,\n",
       "         0.70424886,  0.34348407,  0.08193557,  0.73959105, -0.16329023,\n",
       "        -0.27212709,  0.31208437,  0.51141764, -0.0050305 ,  0.11954339,\n",
       "         0.25752024, -0.08659142,  0.01123745, -0.01286996,  0.56364143,\n",
       "        -0.13980124,  0.28986611, -0.31313673,  0.56979489,  0.0540012 ,\n",
       "         1.03165817,  0.37036491,  0.12126538,  0.19774262,  1.09156032,\n",
       "         0.06793979,  0.15267238, -0.02914582,  0.00293731,  0.08176616,\n",
       "        -0.07033101,  0.30985919,  0.10954587,  0.07466707,  0.06159794,\n",
       "        -0.09609363, -0.05447392, -0.05424918,  0.06778262,  0.31747316,\n",
       "         0.23715683,  0.06628672, -0.02472113, -0.06441361, -0.41550969,\n",
       "        -0.04822568,  0.411421  , -0.02758315, -0.20924522,  0.12108873,\n",
       "         0.08852634,  0.03916829,  0.04256714,  0.06569745,  0.36562291,\n",
       "         0.3152481 ,  0.10411883,  0.13086382, -0.22751094,  0.20445567,\n",
       "         0.16270969,  0.20190733,  0.04218751,  0.06089557,  0.18667773,\n",
       "         0.01805483,  0.55487367,  0.1150022 ,  0.24710523,  0.2351971 ,\n",
       "        -0.00943831,  0.3323346 , -0.6412912 , -0.20961051,  0.16212017,\n",
       "        -0.01043517,  0.09586204,  0.02517294,  0.05427974,  0.0779994 ,\n",
       "         0.74212838, -0.00886145, -0.52289478,  0.02900392, -0.17182391,\n",
       "        -0.04218884,  0.32064167,  0.14489375,  0.02842931, -0.04240083,\n",
       "         0.29220663,  0.07629544, -0.34897807,  0.18319476,  0.21449544,\n",
       "         0.08267414, -0.16668352,  0.01834952]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8935953\n",
      "1157662\n"
     ]
    }
   ],
   "source": [
    "print (len(y_pred_test))\n",
    "print (sum(y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "print (len(y_pred_train))\n",
    "print (sum(y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 28s, sys: 461 ms, total: 1min 29s\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## 4: w_l1 ##\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "num_train = int(round(.8*len(edges_pos)))\n",
    "for e in edges_pos[:num_train]:\n",
    "    emb1 = emb_dict[int(e[0])]\n",
    "    emb2 = emb_dict[int(e[1])]\n",
    "    w = 1 #G_sub20[int(e[0])][int(e[1])]['weight']\n",
    "    y_train.append(w)\n",
    "    res = w_l1(emb1, emb2)\n",
    "    X_train.append(res)\n",
    "#\n",
    "for e in edges_pos[num_train:]:\n",
    "    emb1 = emb_dict[int(e[0])]\n",
    "    emb2 = emb_dict[int(e[1])]\n",
    "    w = 1 \n",
    "    y_test.append(w)\n",
    "    res = w_l1(emb1, emb2)\n",
    "    X_test.append(res)\n",
    "#\n",
    "for e in edges_neg[:num_train]:\n",
    "    emb1 = emb_dict[int(e[0])]\n",
    "    emb2 = emb_dict[int(e[1])]\n",
    "    w = 0 \n",
    "    y_train.append(w)\n",
    "    res = w_l1(emb1, emb2)\n",
    "    X_train.append(res)\n",
    "#\n",
    "for e in edges_neg[num_train:]:\n",
    "    emb1 = emb_dict[int(e[0])]\n",
    "    emb2 = emb_dict[int(e[1])]\n",
    "    w = 0 \n",
    "    y_test.append(w)\n",
    "    res = w_l1(emb1, emb2)\n",
    "    X_test.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olga3993/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression (w_l1): test\n",
      "Precision: 0.7389987293697482\n",
      "Accuracy: 0.856237516650356\n",
      "F-1 (macro): 0.8557201762127032\n",
      "F-1 (micro): 0.856237516650356\n",
      "Logloss: 4.965492713458297\n",
      "ROC-AUC: 0.8786486595054152\n",
      "Logistic regression (w_l1): train\n",
      "Precision: 0.8406465788758758\n",
      "Accuracy: 0.8349246343256069\n",
      "F-1 (macro): 0.834912989522812\n",
      "F-1 (micro): 0.8349246343256069\n",
      "Logloss: 5.701563782484556\n",
      "ROC-AUC: 0.8349246343256069\n",
      "CPU times: user 1min 52s, sys: 3.58 s, total: 1min 55s\n",
      "Wall time: 1min 48s\n",
      "Parser   : 211 ms\n"
     ]
    }
   ],
   "source": [
    "    %%time\n",
    "    # LogReg\n",
    "    precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train,f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train,roc_auc_test, y_pred_train, y_pred_test  = model_logistic(X_train, y_train, X_test, y_test)\n",
    "    print(\"Logistic regression (w_l1): test\")\n",
    "    print('Precision: ' + str(precision_test))\n",
    "    print('Accuracy: ' + str(accuracy_test))\n",
    "    print('F-1 (macro): ' + str(f1_macro_test))\n",
    "    print('F-1 (micro): ' + str(f1_micro_test))\n",
    "    print('Logloss: ' + str(logloss_test))\n",
    "    print('ROC-AUC: ' + str(roc_auc_test))\n",
    "    print(\"Logistic regression (w_l1): train\")\n",
    "    print('Precision: ' + str(precision_train))\n",
    "    print('Accuracy: ' + str(accuracy_train))\n",
    "    print('F-1 (macro): ' + str(f1_macro_train))\n",
    "    print('F-1 (micro): ' + str(f1_micro_train))\n",
    "    print('Logloss: ' + str(logloss_train))\n",
    "    print('ROC-AUC: ' + str(roc_auc_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 12s, sys: 8.75 s, total: 2min 21s\n",
      "Wall time: 2min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train2 =[]\n",
    "for x in X_train:\n",
    "    X_train2.append(list(np.array(x)**2))\n",
    "\n",
    "X_test2 =[]\n",
    "for x in X_test:\n",
    "    X_test2.append(list(np.array(x)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olga3993/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression (w_l2): test\n",
      "Precision: 0.7069161371525676\n",
      "Accuracy: 0.8323081221696751\n",
      "F-1 (macro): 0.8321121950623813\n",
      "F-1 (micro): 0.8323081221696751\n",
      "Logloss: 5.79200456803233\n",
      "ROC-AUC: 0.8590313512818832\n",
      "Logistic regression (w_l2): train\n",
      "Precision: 0.8205244715921372\n",
      "Accuracy: 0.8321092819890245\n",
      "F-1 (macro): 0.8320544335809745\n",
      "F-1 (micro): 0.8321092819890246\n",
      "Logloss: 5.798814315676781\n",
      "ROC-AUC: 0.8321092819890245\n",
      "CPU times: user 3min 38s, sys: 6.65 s, total: 3min 45s\n",
      "Wall time: 3min 37s\n"
     ]
    }
   ],
   "source": [
    "    %%time\n",
    "    # LogReg\n",
    "    precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train,f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train,roc_auc_test, y_pred_train, y_pred_test  = model_logistic(X_train2, y_train, X_test2, y_test)\n",
    "    print(\"Logistic regression (w_l2): test\")\n",
    "    print('Precision: ' + str(precision_test))\n",
    "    print('Accuracy: ' + str(accuracy_test))\n",
    "    print('F-1 (macro): ' + str(f1_macro_test))\n",
    "    print('F-1 (micro): ' + str(f1_micro_test))\n",
    "    print('Logloss: ' + str(logloss_test))\n",
    "    print('ROC-AUC: ' + str(roc_auc_test))\n",
    "    print(\"Logistic regression (w_l2): train\")\n",
    "    print('Precision: ' + str(precision_train))\n",
    "    print('Accuracy: ' + str(accuracy_train))\n",
    "    print('F-1 (macro): ' + str(f1_macro_train))\n",
    "    print('F-1 (micro): ' + str(f1_micro_train))\n",
    "    print('Logloss: ' + str(logloss_train))\n",
    "    print('ROC-AUC: ' + str(roc_auc_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25min 13s, sys: 24.4 s, total: 25min 37s\n",
      "Wall time: 25min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## 2: nw_l1 ##\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "num_train = int(round(.4*len(edges_pos)))\n",
    "for e in edges_pos[:num_train]:\n",
    "    emb1 = emb_dict[int(e[0])]\n",
    "    emb2 = emb_dict[int(e[1])]\n",
    "    w = 1 #G_sub20[int(e[0])][int(e[1])]['weight']\n",
    "    y_train.append(w)\n",
    "    res = nw_l1(emb1, emb2, G,int(e[0]),int(e[1]),emb_dict)\n",
    "    X_train.append(res)\n",
    "#\n",
    "for e in edges_pos[num_train:]:\n",
    "    emb1 = emb_dict[int(e[0])]\n",
    "    emb2 = emb_dict[int(e[1])]\n",
    "    w = 1 \n",
    "    y_test.append(w)\n",
    "    res = nw_l1(emb1, emb2, G,int(e[0]),int(e[1]),emb_dict)\n",
    "    X_test.append(res)\n",
    "#\n",
    "for e in edges_neg[:num_train]:\n",
    "    emb1 = emb_dict[int(e[0])]\n",
    "    emb2 = emb_dict[int(e[1])]\n",
    "    w = 0 \n",
    "    y_train.append(w)\n",
    "    res = nw_l1(emb1, emb2, G,int(e[0]),int(e[1]),emb_dict)\n",
    "    X_train.append(res)\n",
    "#\n",
    "for e in edges_neg[num_train:]:\n",
    "    emb1 = emb_dict[int(e[0])]\n",
    "    emb2 = emb_dict[int(e[1])]\n",
    "    w = 0 \n",
    "    y_test.append(w)\n",
    "    res = nw_l1(emb1, emb2, G,int(e[0]),int(e[1]),emb_dict)\n",
    "    X_test.append(res) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olga3993/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression (nw_l1): test\n",
      "Precision: 0.9851604618531395\n",
      "Accuracy: 0.9920069565431276\n",
      "F-1 (macro): 0.9919677510736085\n",
      "F-1 (micro): 0.9920069565431276\n",
      "Logloss: 0.2760754939950439\n",
      "ROC-AUC: 0.9924101771268805\n",
      "Logistic regression (nw_l1): train\n",
      "Precision: 0.9926552585135976\n",
      "Accuracy: 0.9914645643960297\n",
      "F-1 (macro): 0.9914645519313676\n",
      "F-1 (micro): 0.9914645643960297\n",
      "Logloss: 0.2948064310798988\n",
      "ROC-AUC: 0.9914645643960297\n",
      "CPU times: user 1min 20s, sys: 8.22 s, total: 1min 29s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "    %%time\n",
    "    # LogReg\n",
    "    precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train,f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train,roc_auc_test, y_pred_train, y_pred_test  = model_logistic(X_train, y_train, X_test, y_test)\n",
    "    print(\"Logistic regression (nw_l1): test\")\n",
    "    print('Precision: ' + str(precision_test))\n",
    "    print('Accuracy: ' + str(accuracy_test))\n",
    "    print('F-1 (macro): ' + str(f1_macro_test))\n",
    "    print('F-1 (micro): ' + str(f1_micro_test))\n",
    "    print('Logloss: ' + str(logloss_test))\n",
    "    print('ROC-AUC: ' + str(roc_auc_test))\n",
    "    print(\"Logistic regression (nw_l1): train\")\n",
    "    print('Precision: ' + str(precision_train))\n",
    "    print('Accuracy: ' + str(accuracy_train))\n",
    "    print('F-1 (macro): ' + str(f1_macro_train))\n",
    "    print('F-1 (micro): ' + str(f1_micro_train))\n",
    "    print('Logloss: ' + str(logloss_train))\n",
    "    print('ROC-AUC: ' + str(roc_auc_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 18s, sys: 9.99 s, total: 2min 28s\n",
      "Wall time: 2min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train2 =[]\n",
    "for x in X_train:\n",
    "    X_train2.append(list(np.array(x)**2))\n",
    "\n",
    "X_test2 =[]\n",
    "for x in X_test:\n",
    "    X_test2.append(list(np.array(x)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olga3993/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression (nw_l2): test\n",
      "Precision: 0.984098067256659\n",
      "Accuracy: 0.9916598889926415\n",
      "F-1 (macro): 0.9916197641916635\n",
      "F-1 (micro): 0.9916598889926415\n",
      "Logloss: 0.28806318859140984\n",
      "ROC-AUC: 0.9921120551101485\n",
      "Logistic regression (nw_l2): train\n",
      "Precision: 0.9919908634776916\n",
      "Accuracy: 0.9918831321312084\n",
      "F-1 (macro): 0.9918831320339117\n",
      "F-1 (micro): 0.9918831321312084\n",
      "Logloss: 0.2803498856885695\n",
      "ROC-AUC: 0.9918831321312084\n",
      "CPU times: user 2min 49s, sys: 8.33 s, total: 2min 58s\n",
      "Wall time: 2min 50s\n"
     ]
    }
   ],
   "source": [
    "    %%time\n",
    "    # LogReg\n",
    "    precision_train, precision_test, accuracy_train, accuracy_test, f1_macro_train,f1_macro_test, f1_micro_train, f1_micro_test, logloss_train, logloss_test, roc_auc_train,roc_auc_test, y_pred_train, y_pred_test  = model_logistic(X_train2, y_train, X_test2, y_test)\n",
    "    print(\"Logistic regression (nw_l2): test\")\n",
    "    print('Precision: ' + str(precision_test))\n",
    "    print('Accuracy: ' + str(accuracy_test))\n",
    "    print('F-1 (macro): ' + str(f1_macro_test))\n",
    "    print('F-1 (micro): ' + str(f1_micro_test))\n",
    "    print('Logloss: ' + str(logloss_test))\n",
    "    print('ROC-AUC: ' + str(roc_auc_test))\n",
    "    print(\"Logistic regression (nw_l2): train\")\n",
    "    print('Precision: ' + str(precision_train))\n",
    "    print('Accuracy: ' + str(accuracy_train))\n",
    "    print('F-1 (macro): ' + str(f1_macro_train))\n",
    "    print('F-1 (micro): ' + str(f1_micro_train))\n",
    "    print('Logloss: ' + str(logloss_train))\n",
    "    print('ROC-AUC: ' + str(roc_auc_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6813712/6813712 [56:12<00:00, 2020.56it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39min 10s, sys: 33.3 s, total: 39min 43s\n",
      "Wall time: 56min 12s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "f = open('X_train_nw_l2', 'w')                                \n",
    "for x in tqdm(X_train):\n",
    "    line = \"\"\n",
    "    for i in x:\n",
    "        line += str(i)\n",
    "        line += \" \"\n",
    "    f.write(line+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2122304/2122304 [16:22<00:00, 2161.12it/s]\n"
     ]
    }
   ],
   "source": [
    "f = open('X_test_nw_l2', 'w')                                \n",
    "for x in tqdm(X_test):\n",
    "    line = \"\"\n",
    "    for i in x:\n",
    "        line += str(i)\n",
    "        line += \" \"\n",
    "    f.write(line+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6813712/6813712 [00:14<00:00, 470086.03it/s]\n",
      "100%|██████████| 2122304/2122304 [00:04<00:00, 521256.21it/s]\n"
     ]
    }
   ],
   "source": [
    "f = open('y_train_nw_l2', 'w')                                \n",
    "for y in tqdm(y_train):\n",
    "    line = str(y)\n",
    "    f.write(line+'\\n')\n",
    "f.close()\n",
    "\n",
    "f = open('y_test_nw_l2', 'w')                                \n",
    "for y in tqdm(y_test):\n",
    "    line = str(y)\n",
    "    f.write(line+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
